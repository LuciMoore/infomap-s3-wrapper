#! /bin/bash
# 
# SUN NOV 25 @ 5pm
#   Eric Earl modified batch_subject_submitter.sh to generalize to other batch needs
# 
# WED NOV 28 @ 10am
#   Eric Earl further modified batch_subject_conductor.sh to make this an easy fill-in-the-blank template
# 

while true ; do

  # meta settings
  DESIRED_QUEUE_SIZE=30 # number of continuous jobs
  SUBMISSION_BATCH_SIZE=10 # max size of batch to submit at once
  INTERBATCH_MINUTES=5 # time between batches in minutes
  QUEUING_THRESHOLD=`echo $DESIRED_QUEUE_SIZE - $SUBMISSION_BATCH_SIZE | bc` # internal varibale

  # sbatch settings
  JOB_NAME=VAMETH # a short, but descriptive job name
  PARTITION=amd512,amdsmall,amdlarge,ram256g # the Exacloud SLURM partition for the jobs, for example "highio" or "exacloud"
  NCPUS=2 # number of cores per SBATCH
  MEM_PER_CPU=53G # memory (RAM) per core, so if NCPUS=4 and MEM_PER_CPU=2G, the job overall would get 8GB total memory
  TIME_LIMIT="4:00:00" # a SLURM time-limit complete string in "quotes"
  TEMP_SPACE=150GB

  # absolute path to the main place for logs per subject
  LOG_DIR=/home/faird/shared/projects/AnitaOHSUVAcollab/template_matching_combined_sessions/${JOB_NAME}_logs

  # absolute path to the main script to execute (without arguments)
  #SBATCH_SCRIPT=/home/exacloud/lustre1/fnl_lab/projects/ABCD/submit_bids_app/TEMPLATE_script.sh
  SBATCH_SCRIPT=/home/faird/shared/projects/AnitaOHSUVAcollab/code/sync_then_twins_mapping_wrapper_VA.sh

  # absolute path to the list file of subjects to submit, one subject ID per line
  SUBJECT_LIST_FILE=/home/faird/shared/projects/AnitaOHSUVAcollab/template_matching_combined_sessions/SUBIDs

  # absolute path to the list file of submitted subjects which should be ignored in further submissions, one subject ID per line in the same format as the SUBJECT_LIST_FILE
  submitted_subjects_file=$LOG_DIR/${JOB_NAME}_$(basename ${SUBJECT_LIST_FILE})

  SHORT_JOB_NAME=`echo $JOB_NAME | sed 's|\(.\{8\}\).*|\1|g'` # internal varibale
  mkdir -p $LOG_DIR # make the log dir
  chmod g+rw $LOG_DIR # make the log dir group+read/write
  touch $submitted_subjects_file # if the file doesn't exist it is created, otherwise, just updated modified time
  
  # makes BASH array of subjects from the difference of the main subject list and the already-submitted subject list
  subjects=( $( cat $SUBJECT_LIST_FILE | grep -vf $submitted_subjects_file ) )

  # check for jobs in the queue with this job's short 8-character name and...
  # if the count is above the "queuing threshold"...
  while [ $(squeue -p  $PARTITION | grep $SHORT_JOB_NAME | wc -l) -gt $QUEUING_THRESHOLD ]; do
    sleep 5m # then wait 5 minutes before checking again in a while loop
  done

  # submit next list of subjects
  let i=0 # start at 0

  # for each subject in the array
  for subject in ${subjects[@]}; do
    logdir=$LOG_DIR/$subject
    mkdir -p "$logdir"

    #####################################
    ### vvv SCRIPT-SPECIFIC STUFF vvv ###
    #                                   #
    # add here what you need here to    #
    # pass the appropriate arguments    #
    # per subject to your script        #
    #                                   #
    ### vvv    GENERIC EXAMPLE    vvv ###
    # ARG1=SOMETHING
    # ARG2=ANYTHING
    # ARG3=WHATEVER_ELSE
    # sbatch --job-name $JOB_NAME --partition $PARTITION --cpus-per-task $NCPUS --mem-per-cpu $MEM_PER_CPU --time $TIME_LIMIT --output $logdir/${subject}.out --error $logdir/${subject}.err $SBATCH_SCRIPT $ARG1 $ARG2 $ARG3 2>> $LOG_DIR/jobid_list.txt &
    #srun  -A fnl_lab --exclude=exanode-1-56 --time=36:00:00  --cpus-per-task=4  --mem-per-cpu=50GB -e /home/exacloud/lustre1/fnl_lab/projects/ABCD_net_template_matching/slurm_err_out/sub-NDARINVELK0KFTA.err -o /home/exacloud/lustre1/fnl_lab/projects/ABCD_net_template_matching/slurm_err_out/sub-NDARINVELK0KFTA.out -n 1 /home/exacloud/lustre1/fnl_lab/code/internal/analyses/compare_matrices/twins_mapping_wrapper.sh /home/groups/brainmri/abcd/data/HCP/derivatives/dcan_abcd_pipeline/sub-NDARINVELK0KFTA/ses-baselineYear1Arm1/func/sub-NDARINVELK0KFTA_ses-baselineYear1Arm1_task-rest_bold_timeseries.dtseries.nii /home/groups/brainmri/abcd/data/HCP/derivatives/dcan_abcd_pipeline/sub-NDARINVELK0KFTA/ses-baselineYear1Arm1/func/sub-NDARINVELK0KFTA_ses-baselineYear1Arm1_task-rest_bold_mask.mat /home/groups/brainmri/abcd/data/HCP/derivatives/dcan_abcd_pipeline/sub-NDARINVELK0KFTA/ses-baselineYear1Arm1/anat/sub-NDARINVELK0KFTA_ses-baselineYear1Arm1_hemi-L_atlas-MNI_space-fsLR32k_midthickness.surf.gii /home/groups/brainmri/abcd/data/HCP/derivatives/dcan_abcd_pipeline/sub-NDARINVELK0KFTA/ses-baselineYear1Arm1/anat/sub-NDARINVELK0KFTA_ses-baselineYear1Arm1_hemi-R_atlas-MNI_space-fsLR32k_midthickness.surf.gii sub-NDARINVELK0KFTA_templ_match_to_ABCD164 /home/exacloud/lustre1/fnl_lab/projects/ABCD_net_template_matching/ABCD_twins_dscalars &
    
    #derivatves_path=/home/groups/brainmri/abcd/data/HCP/derivatives/dcan_abcd_pipeline

    subjectID="$(cut -d'_' -f1 <<< "$subject")"
    session="$(cut -d'_' -f2 <<< "$subject")"


    derivatves_path=/home/faird/shared/projects/AnitaOHSUVAcollab/derivatives/abcd-hcp-pipeline
    #session=baselineYear1Arm1
    session=ses-combined
    outputname=_templ_match_to_ABCD164
    outputfolder=/home/faird/shared/projects/AnitaOHSUVAcollab/template_matching_combined_sessions/TM_networks
    
    #dtseries_suffix=_ses-baselineYear1Arm1_task-rest_bold_timeseries.dtseries.nii
    #dtseries_suffix=_ses-baselineYear1Arm1_task-rest_bold_desc-filtered_timeseries.dtseries.nii
       #dtseries_suffix=_ses-baselineYear1Arm1_task-rest_bold_desc-filtered_timeseries.dtseries.nii
    dtseries_suffix=_merged_timeseries.dtseries.nii
    #dscalar_suffix=_ses-baselineYear1Arm1_task-rest_bold_timeseries_template_matched_Zscored_recolored.dscalar.nii
    #dconn_suffix=ses-both_merged_tasks_SMOOTHED_2.55.dtseries.nii_all_frames_at_FD_0.2_merged_additional_mask_half1.dconn.nii
    dconn_suffix=ses-both_merged_timeseries_SMOOTHED_2.55.dtseries.nii_all_frames_at_FD_0.2.dconn.nii
    #dconn_bucket_destination=s3://Cohen_Lab_ADHD_MedChal_for_DCAN_lab/dconns/half1
    #motion_suffix=_ses-baselineYear1Arm1_task-rest_bold_mask.mat
    motion_suffix=_merged_motion.mat
    #use premade mask to match the invidividual specific networks.
    #motion_folder=/home/rando149/shared/projects/ABCD_net_template_matching/ABCD_templ_matched_scalars_group1_10_min_surface_only_motion_masks
    motion_folder=${derivatves_path}/${subjectID}/${session}/func
    #motion_suffix=_ses-baselineYear1Arm1_task-rest_bold_masksurf_only.mat_0.2_cifti_censor_FD_vector_10_minutes_of_data_at_0.2_threshold.txt

    #L_surf_suffix=_ses-baselineYear1Arm1_hemi-L_atlas-MNI_space-fsLR32k_midthickness.surf.gii
    #R_surf_suffix=_ses-baselineYear1Arm1_hemi-R_atlas-MNI_space-fsLR32k_midthickness.surf.gii
    #L_surf_suffix=_ses-baselineYear1Arm1_hemi-L_space-MNI_mesh-fsLR32k_midthickness.surf.gii
    #R_surf_suffix=_ses-baselineYear1Arm1_hemi-R_space-MNI_mesh-fsLR32k_midthickness.surf.gii
    L_surf_suffix=_hemi-L_space-MNI_mesh-fsLR32k_midthickness.surf.gii
    R_surf_suffix=_hemi-R_space-MNI_mesh-fsLR32k_midthickness.surf.gii
    #expected_output_suffix=_ses-baselineYear1Arm1_task-rest_bold_timeseries_template_matched_Zscored_recolored.dscalar.nii
    #expected_info_output_suffix=_ses-baselineYear1Arm1_task-rest_bold_timeseries_infomap_densities_recolored.dscalar.nii
    
    #expected_output_suffix=${session}_task-rest_bold_desc-filtered_timeseries_template_matched_recolored.dscalar.nii
    #expected_info_output_suffix=_ses-baselineYear1Arm1_task-rest_bold_timeseries_infomap_densities_recolored.dscalar.nii
    expected_output_suffix=${session}_merged_timeseries_template_matched_Zscored_recolored.dscalar.nii
    
    FD_threshold=0.2
    TR=0.8
    #TR=2    
    #minutes_limit=none
    minutes_limit=none
    transform_data=Convert_to_Zscores
    #transform_data=Convert_FisherZ_to_r
    #template_path=/home/faird/shared/code/internal/analytics/compare_matrices_to_assign_networks/support_files/seedmaps_ABCD161template_SMOOTHED_dtseries_SurfOnly_all_networks.mat
    template_path=/home/faird/shared/code/internal/analytics/compare_matrices_to_assign_networks/support_files/seedmaps_ABCD164template_SMOOTHED_dtseries_all_networksZscored.mat
    surface_only=0
    already_surface_only=0
    use_all_ABCD_tasks=0
    run_infomap_too=0
    wb_command_path=/home/feczk001/shared/code/external/utilities/workbench/1.4.2/workbench/bin_rh_linux64/wb_command
    #working_folder=/home/rando149/shared/projects/ABCD_laterality/ABCD_templ_matched_scalars_group1_10_min_surface_only_temp
    working_folder=/tmp
    dtseries_conc=${derivatves_path}/$subjectID/$session/func/${subjectID}_${session}${dtseries_suffix}
    use_continous_minutes=0
    memory_limit_value=64
    clean_up_intermediate_files=1
    additional_mask=none
    remove_outliers=1
echo "checking for subject" ${outputfolder}/"$subjectID"_"${expected_output_suffix}"


  if [ -f ${outputfolder}/"$subjectID"_"${expected_output_suffix}" ]; then #check for output files
	#if [ -f ${outputfolder}/$subject${expected_output_suffix} ] && [ -f ${outputfolder}/$subject${expected_info_output_suffix} ]; then #check for output files
		echo "Final template matching and infomap .dscalar.nii file found for " "$subjectID" ". Sbatch request will not be made for subjectID. " >> $LOG_DIR/jobid_list.txt &
		echo "Final template matching and infomap .dscalar.nii file found for " "$subjectID" ". Sbatch request will not be made for subject. "
	elif [ ! -d ${derivatves_path}/"$subjectID" ]; then # check that the subject folder even exists,
		echo $subjectID "folder was not found in derivatives." >> $LOG_DIR/jobid_list.txt &
		echo $subjectID "folder was not found in derivatives."
	elif [ -z "$(ls -A  ${derivatves_path}/"$subjectID"/"$session"/func)" ]; then  #check that therer's anything in subject's folder.
		echo $subjectID "folder was found in derivatives folder but is empty." >> $LOG_DIR/jobid_list.txt &
		echo $subjectID "folder was found in derivatives folder but is empty."
	else

	#if [ -f ${outputfolder}/$subject${expected_output_suffix} ]; then #check for output files
	#	echo "Final template matching .dscalar.nii file found for " $subject ". Sbatch request will not be made for subject. " >> $LOG_DIR/jobid_list.txt &
	#	echo "Final template matching .dscalar.nii file found for " $subject ". Sbatch request will not be made for subject. "
	#else
    		echo "Submitting job for " $subjectID
    		#sbatch -A fnl_lab --job-name $JOB_NAME --partition $PARTITION --cpus-per-task $NCPUS --mem-per-cpu $MEM_PER_CPU --time $TIME_LIMIT --exclude=`cat /home/exacloud/lustre1/fnl_lab/projects/ABCD/submitters/exclude_list.csv` --output $logdir/$subject.out --error $logdir/$subject.err $SBATCH_SCRIPT ${derivatves_path}/$subject/ses-$session/func/$subject${dtseries_suffix} ${derivatves_path}/$subject/ses-baselineYear1Arm1/func/$subject${motion_suffix} ${derivatves_path}/$subject/ses-baselineYear1Arm1/anat/$subject${L_surf_suffix} ${derivatves_path}/$subject/ses-baselineYear1Arm1/anat/$subject${R_surf_suffix} $subject${outputname} ${outputfolder} ${TR} ${minutes_limit} ${FD_threshold} ${transform_data} ${template_path} ${surface_only} ${already_surface_only} ${use_all_ABCD_tasks} ${run_infomap_too} >> $LOG_DIR/jobid_list.txt &

    		#sg rando149 -c "sbatch --job-name $JOB_NAME --partition $PARTITION --cpus-per-task $NCPUS --mem-per-cpu $MEM_PER_CPU --time ${TIME_LIMIT} --output $logdir/${subjectID}_${session}.out --error $logdir/${subjectID}_${session}.err $SBATCH_SCRIPT ${derivatves_path}/${subjectID}/$session/func/${subjectID}_${session}${dtseries_suffix} ${motion_folder}/${subjectID}_${session}${motion_suffix} ${derivatves_path}/${subjectID}/${session}/anat/${subjectID}_${session}${L_surf_suffix} ${derivatves_path}/${subjectID}/${session}/anat/${subjectID}_${session}${R_surf_suffix} ${subjectID}_${session}${outputname} ${outputfolder} ${TR} ${minutes_limit} ${FD_threshold} ${transform_data} ${template_path} ${surface_only} ${already_surface_only} ${use_all_ABCD_tasks} ${run_infomap_too} ${working_folder} ${dtseries_conc} ${use_continous_minutes} ${memory_limit_value} ${clean_up_intermediate_files} ${wb_command_path} ${additional_mask} ${subjectID} ${dconn_suffix} ${working_folder} ${dconn_bucket_destination} >> $LOG_DIR/jobid_list.txt &"
     		sg rando149 -c "sbatch --job-name $JOB_NAME --partition $PARTITION --cpus-per-task $NCPUS --mem-per-cpu $MEM_PER_CPU --time ${TIME_LIMIT} --tmp=${TEMP_SPACE} --output $logdir/${subjectID}_${session}.out --error $logdir/${subjectID}_${session}.err $SBATCH_SCRIPT  ${subjectID} ${session} ${derivatves_path} ${working_folder} ${subjectID}_${session}${outputname} ${outputfolder} ${TR} ${minutes_limit} ${FD_threshold} ${transform_data} ${template_path} ${surface_only} ${already_surface_only} ${use_all_ABCD_tasks} ${run_infomap_too} ${working_folder} ${dtseries_conc} ${use_continous_minutes} ${memory_limit_value} ${clean_up_intermediate_files} ${wb_command_path} ${additional_mask} ${remove_outliers} >> $LOG_DIR/jobid_list.txt &"
 
  fi

    ### ^^^    GENERIC EXAMPLE    ^^^ ###
    #                                   #
    ### ^^^ SCRIPT-SPECIFIC STUFF ^^^ ###
    #####################################

    echo $subjectID >> $submitted_subjects_file # append this subject to the submitted subjects file to ignore in the next round
    sleep 3s # pause 5 seconds just in case
    SUBJECTCOUNT=`cat $SUBJECT_LIST_FILE | wc -l`
    SUBMITTEDCOUNT=`cat $submitted_subjects_file | wc -l`

    let i=i+1 # increment for each subject you submit
    if [ $i -eq $SUBMISSION_BATCH_SIZE ] || [ $SUBJECTCOUNT -eq $SUBMITTEDCOUNT ]; then
      break # stop when you reach the submission batch size
    fi

  done

  if [ $SUBJECTCOUNT -eq $SUBMITTEDCOUNT ] ; then
    break
  else
    sleep ${INTERBATCH_MINUTES}m # wait some amount of minutes before starting this loop over and doing the next submission batch
	echo "Checking for old files...and removing them."
	#/home/exacloud/lustre1/fnl_lab/projects/ABCD_net_template_matching_task/clean_up_ADHD_fails.sh
  /home/rando149/shared/projects/ABCD_laterality/clean_up_ADHD_fails.sh ${working_folder}
  fi

done
chmod g+rw -R ${LOG_DIR} 2>/dev/null

